./alexnet train -batchsize 120 -epochs 8  -save ./temp.weights 
batch size: 120 
epochs: 8 


>>>>>>>>>>>>>>>>>>>>>>>>>>> training begin >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
-----------------------------1---------------------------------
forward_alexnet duration: 11.8323s 
pred[ 21 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 196 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 21 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 367 196 367 367 367 367 367 367 367 367 367 ]  label[ 3 4 2 6 6 6 2 2 6 0 9 6 5 8 6 1 7 8 1 7 3 3 4 4 3 2 0 5 7 0 0 2 5 0 9 5 1 9 7 1 2 7 1 3 7 0 7 5 8 7 8 0 8 3 7 4 2 0 8 1 8 1 2 9 4 2 8 0 6 4 3 4 3 1 9 3 6 9 7 8 9 7 5 2 7 6 9 3 0 7 3 6 2 8 7 5 0 8 1 8 1 9 7 9 1 9 8 2 9 8 4 9 3 0 9 9 3 5 4 0 ]
cross entropy loss on batch data is 7.274540 
backward_alexnet duration: 2.8843s 
-----------------------------1---------------------------------
-----------------------------2---------------------------------
forward_alexnet duration: 11.9249s 
pred[ 367 455 455 584 607 367 367 367 367 367 188 455 367 367 455 486 455 455 367 777 455 455 313 455 367 455 367 367 455 367 455 367 455 367 367 367 367 455 367 367 367 367 421 455 367 455 367 455 367 367 367 367 367 455 367 455 455 455 367 455 188 367 455 367 367 367 455 367 367 455 777 390 367 455 367 367 455 455 188 455 367 367 336 367 455 455 455 455 367 455 455 455 584 777 188 367 455 367 455 21 367 21 21 367 21 21 21 21 553 21 21 21 21 21 21 21 455 367 21 21 ]  label[ 3 5 0 6 5 3 1 3 7 7 7 3 5 9 8 6 5 0 8 1 3 0 3 8 2 7 4 2 5 7 2 4 5 0 9 2 6 1 4 5 3 2 4 0 4 8 1 5 4 4 5 5 8 0 5 7 5 6 4 2 7 4 2 8 5 8 6 4 4 7 0 9 2 7 4 8 2 6 8 6 6 7 1 2 5 7 4 5 3 3 3 0 8 9 7 5 2 9 3 6 5 3 2 3 4 3 4 1 5 5 2 3 3 0 2 0 9 7 2 8 ]
cross entropy loss on batch data is 7.378562 
backward_alexnet duration: 2.5115s 
-----------------------------2---------------------------------
-----------------------------3---------------------------------
forward_alexnet duration: 11.7555s 
pred[ 367 188 455 455 777 390 367 455 390 777 424 455 455 455 455 777 777 455 390 188 455 188 455 455 777 455 455 455 455 455 455 455 455 188 455 455 188 777 455 455 777 367 455 561 932 188 455 188 367 455 455 455 367 367 390 455 367 777 367 455 196 455 455 188 367 188 455 196 367 777 455 455 188 455 367 455 455 814 455 455 390 455 188 455 455 455 390 584 455 21 455 188 21 21 21 21 21 21 21 21 21 21 21 367 21 21 367 21 21 21 21 21 21 21 21 21 21 21 21 390 ]  label[ 7 2 4 5 8 8 6 4 7 6 0 4 4 3 7 7 6 5 8 6 1 1 6 8 8 3 7 1 5 8 8 0 9 3 6 7 5 0 9 2 7 9 5 1 0 9 4 7 0 3 8 9 3 2 7 3 8 2 2 1 2 1 8 9 9 7 2 2 2 0 0 8 9 4 1 5 6 3 8 6 8 1 2 0 7 6 7 8 9 0 0 8 8 7 9 7 5 6 6 4 1 9 2 7 0 9 4 0 8 0 2 8 6 0 1 9 6 8 2 7 ]
cross entropy loss on batch data is 7.096488 
backward_alexnet duration: 2.4194s 
-----------------------------3---------------------------------
-----------------------------4---------------------------------
forward_alexnet duration: 11.8954s 
pred[ 21 511 21 26 21 196 367 21 21 21 21 21 21 21 320 367 21 21 367 21 390 196 196 21 21 390 21 196 21 196 21 26 21 21 367 188 21 390 188 390 584 390 390 390 188 188 367 390 455 390 188 455 390 188 188 777 188 188 188 777 390 367 390 188 367 21 390 21 21 390 21 367 21 21 21 21 21 367 21 21 21 21 21 21 367 196 21 390 367 196 21 400 367 390 21 455 390 188 390 390 367 367 390 390 455 188 390 390 390 229 390 424 390 390 390 455 390 188 188 400 ]  label[ 1 4 7 4 0 0 2 1 9 3 4 4 7 1 8 3 4 9 5 0 2 8 1 4 7 5 3 5 8 9 0 5 6 5 1 6 4 8 0 5 2 8 3 5 4 2 5 6 1 9 4 1 4 9 5 9 2 6 8 5 5 3 4 2 8 5 8 7 1 3 9 3 7 3 8 6 0 7 2 7 1 6 3 0 1 5 3 1 9 5 1 4 3 1 6 3 6 4 4 9 2 1 6 2 0 6 4 6 7 1 4 7 9 8 1 6 9 1 9 6 ]
cross entropy loss on batch data is 7.294686 
backward_alexnet duration: 2.4186s 
-----------------------------4---------------------------------
-----------------------------5---------------------------------
forward_alexnet duration: 11.7598s 
pred[ 196 511 21 858 287 460 21 367 21 196 511 196 196 390 26 390 390 21 390 681 196 196 196 390 21 390 196 390 390 188 188 390 584 196 196 188 196 26 196 196 188 26 390 21 607 26 188 390 858 460 460 21 681 196 21 367 21 511 21 196 26 367 188 196 196 196 455 390 455 390 196 390 455 400 390 607 390 390 390 367 188 196 390 607 196 26 21 455 367 400 390 814 188 390 390 196 196 21 858 681 196 262 21 21 26 858 511 858 858 858 511 858 858 858 858 858 858 858 511 858 ]  label[ 8 5 3 0 2 5 8 1 0 4 9 2 7 9 1 3 1 1 7 2 4 7 8 2 8 8 3 3 9 3 1 5 9 3 2 1 2 8 1 7 2 9 0 4 9 1 5 6 9 2 0 8 0 1 8 3 9 8 8 1 8 1 7 4 8 4 1 0 4 2 6 2 3 3 5 7 9 0 8 2 5 2 5 8 9 0 4 8 2 2 3 4 0 0 7 5 4 1 1 7 6 7 9 8 3 1 8 7 2 1 7 6 9 3 5 8 2 7 1 7 ]
cross entropy loss on batch data is 7.566759 
backward_alexnet duration: 2.4876s 
-----------------------------5---------------------------------
-----------------------------6---------------------------------
forward_alexnet duration: 11.8348s 
pred[ 858 858 858 858 858 858 858 858 858 858 914 858 858 858 858 301 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 301 681 681 196 858 196 858 460 858 858 858 858 681 858 858 196 196 26 196 460 858 858 858 858 858 858 196 858 858 858 858 858 858 858 681 858 26 858 858 858 26 858 612 460 858 858 858 858 511 858 858 858 ]  label[ 6 3 2 8 5 0 5 6 8 3 8 2 8 3 6 5 0 0 2 0 6 7 4 4 2 1 7 0 8 3 1 0 2 3 2 5 4 6 7 2 1 3 1 3 4 8 9 5 3 2 3 0 3 7 5 6 6 3 1 4 8 4 0 1 3 3 4 5 3 2 8 9 0 2 1 6 8 3 2 7 1 9 1 1 2 8 9 4 9 5 4 0 5 1 6 8 2 8 5 9 1 0 1 6 7 9 9 7 5 6 4 1 7 7 9 7 5 5 8 7 ]
cross entropy loss on batch data is 8.258024 
backward_alexnet duration: 2.5018s 
-----------------------------6---------------------------------
-----------------------------7---------------------------------
forward_alexnet duration: 11.6948s 
pred[ 858 511 858 858 858 858 858 858 511 511 858 858 858 858 858 858 858 858 858 858 858 858 858 511 858 858 511 858 301 858 858 301 858 511 858 858 858 858 858 858 858 511 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 511 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 511 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 858 511 858 858 858 858 ]  label[ 7 5 9 3 6 2 3 4 9 3 1 1 1 1 4 5 4 2 6 9 3 3 2 1 5 4 0 9 0 5 4 1 3 2 5 3 7 2 8 6 7 4 3 5 4 0 4 3 0 8 1 0 7 0 9 7 5 8 6 0 7 3 7 6 8 2 9 2 3 0 4 7 4 4 0 4 9 6 7 5 8 9 5 6 2 5 5 1 4 0 8 1 9 5 1 6 6 5 7 1 9 6 8 5 5 6 7 5 3 0 8 2 2 7 4 5 6 9 1 5 ]
cross entropy loss on batch data is 9.228002 
backward_alexnet duration: 2.7421s 
-----------------------------7---------------------------------
-----------------------------8---------------------------------
forward_alexnet duration: 12.3079s 
pred[ 511 511 511 712 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 511 712 511 511 511 511 511 511 511 511 511 511 511 ]  label[ 9 5 7 4 8 2 7 4 9 1 6 9 0 8 0 1 8 3 8 5 1 0 6 8 5 1 7 1 6 4 5 0 5 3 4 7 1 8 0 5 4 4 8 7 9 6 5 2 6 6 6 7 6 6 4 0 5 1 4 1 8 5 0 4 8 7 8 1 7 3 4 8 6 9 1 2 7 5 4 6 3 7 8 6 1 8 5 9 9 1 0 8 7 6 1 7 6 3 7 1 1 6 1 2 5 7 9 2 8 2 4 8 4 0 2 8 3 1 9 2 ]
cross entropy loss on batch data is 11.223410 
backward_alexnet duration: 2.8444s 
-----------------------------8---------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>> training end >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

Save weights to "./temp.weights" successfully... 